---
title: Some Ideas for Improving MCP Client Responses
date: '2025-05-20'
tags: ['AI', 'LLMs']
draft: true
summary: A few things you can try to improve the accuracy of the MCP client responses.
---

## Control

### How do we know if we're making any progress?

We need to be able to set a list of example interactions and then test them as we make changes.

For the purposes of this document, we can use the following example interactions as taken from gcp-mcp server:

1. List available projects:
```
List all GCP projects I have access to
```

2. Select a project:
```
Use project my-project-id
```

3. Check billing status:
```
What's my current billing status?
```

4. View logs:
```
Show me the last 10 log entries from my project
```

### Starting Point

- Model Provider: OLLAMA
- Model: gemma3:4b
- Prompt:

```Jinja2
You are an gcloud CLI expert. Translate the user's natural language query into the appropriate gcloud CLI command based on the available commands.

Available commands:
{{ command_list }}

Only return the suggested command from the available commands and nothing else. 
Do not include any Markdown, formatting or backticks.
```

## 1. Updating the system prompt

Templatising system prompts so that they can be stored in source control.

Used Jinja2 to create a templating system for the system prompt.

Allows for switching between different system prompts based on the use case.


## 2. Using a different model

### Hosted Models

Adopted adapter design pattern to enable interoperability with different LLMs.

#### OpenAI

gpt-3.5-turbo

- Tries to do much, i.e. writes code for executing the task instead of just selecting a command from the given options.

```bash
Translated to: run-gcp-code --code "const {projects} = require('@google-cloud/resource-manager').getProjects(); const projectList = await projects.list(); return projectList;"
```

gpt-4.1

- More accurate than gpt-3.5-turbo.
- Performs well 
- Is able to select the correct command from the given options.

```
Use project ....
```

#### Anthropic

### Local Models


## 3. Tuning hyperparameters

## Conclusion

